import os
import time
import requests
import urllib3
import traceback
import re
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import ollama
from groq import Groq
from huggingface_hub import InferenceClient
from sentence_transformers import CrossEncoder
from openai import OpenAI as OpenAIClient

# Disable SSL warnings for HuggingFace API (common in corporate/proxy environments)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- Configuration ---
# Google Cloud Run compatible - all URLs from environment variables
# For Docker: use service name or localhost (e.g., http://ollama-service:11434)
# For Cloud Run: use full URL if Ollama is deployed separately
OLLAMA_HOST = os.getenv("OLLAMA_HOST", os.getenv("OLLAMA_URL", "http://localhost:11434"))
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
HUGGINGFACE_API_KEY = os.getenv("HUGGINGFACE_API_KEY")
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
ALIBABA_API_KEY = os.getenv("ALIBABA_API_KEY", os.getenv("DASHSCOPE_API_KEY"))

# --- FastAPI App Initialization ---
app = FastAPI(
    title="Model Inference Service",
    description="A microservice to interact with various LLM providers like Ollama and Groq.",
    version="1.0.0"
)

# --- Pydantic Models for API requests ---
class GenerationRequest(BaseModel):
    prompt: str
    model: str
    # Optional parameters
    temperature: float = 0.7
    max_tokens: int = 1024

class GenerationResponse(BaseModel):
    response: str
    model_used: str

class EmbedRequest(BaseModel):
    texts: list[str]
    model: Optional[str] = None

class EmbedResponse(BaseModel):
    embeddings: list[list[float]]
    model_used: str

class PullModelRequest(BaseModel):
    model_name: str

class PullModelResponse(BaseModel):
    status: str
    message: str

class RerankRequest(BaseModel):
    query: str
    documents: List[str]

class RerankResponse(BaseModel):
    results: List[dict]

class GenerateAnswerRequest(BaseModel):
    query: str
    docs: List[dict]
    model: Optional[str] = "llama-3.1-8b-instant"
    max_context_chars: Optional[int] = 8000

# Add the RAG query models
class RAGQueryRequest(BaseModel):
    session_id: str
    query: str
    top_k: int = 5
    use_rerank: bool = True
    min_score: float = 0.1
    max_context_chars: int = 8000
    model: Optional[str] = None

class RAGQueryResponse(BaseModel):
    answer: str
    sources: List[str] = []

class OutputCleanerRequest(BaseModel):
    raw_output: str
    original_query: str
    model: Optional[str] = "llama-3.1-8b-instant"

class OutputCleanerResponse(BaseModel):
    cleaned_answer: str
    model_used: str

# --- LLM Clients ---
ollama_client = None
OLLAMA_AVAILABLE = False
OLLAMA_LAST_FAILURE_TIME = None  # Track when Ollama last failed
OLLAMA_RETRY_COOLDOWN = 300  # Don't retry for 5 minutes after failure

def get_ollama_client():
    """Get or initialize Ollama client with fast timeout - non-blocking."""
    global ollama_client, OLLAMA_AVAILABLE, OLLAMA_LAST_FAILURE_TIME
    
    # If we already know Ollama is unavailable, check cooldown before retrying
    if ollama_client is None and not OLLAMA_AVAILABLE:
        if OLLAMA_LAST_FAILURE_TIME is not None:
            time_since_failure = time.time() - OLLAMA_LAST_FAILURE_TIME
            if time_since_failure < OLLAMA_RETRY_COOLDOWN:
                # Still in cooldown, don't retry
                return None
            # Cooldown expired, reset failure time and try again
            OLLAMA_LAST_FAILURE_TIME = None
    
    if ollama_client is not None and OLLAMA_AVAILABLE:
        try:
            # Quick check if still available with very short timeout
            import threading
            check_result = [None]
            check_exception = [None]
            
            def quick_check():
                try:
                    check_result[0] = ollama_client.list()
                except Exception as e:
                    check_exception[0] = e
            
            check_thread = threading.Thread(target=quick_check)
            check_thread.daemon = True
            check_thread.start()
            check_thread.join(timeout=0.5)  # Very short timeout
            
            if check_thread.is_alive():
                # Ollama is slow/unavailable, mark as unavailable but don't block
                ollama_client = None
                OLLAMA_AVAILABLE = False
                OLLAMA_LAST_FAILURE_TIME = time.time()
                return None
            
            if check_exception[0]:
                # Connection failed, reset
                ollama_client = None
                OLLAMA_AVAILABLE = False
                OLLAMA_LAST_FAILURE_TIME = time.time()
                return None
            
            return ollama_client
        except:
            # Client became unavailable, reset
            ollama_client = None
            OLLAMA_AVAILABLE = False
            return None
    
    # Try to initialize/reconnect with very fast timeout
    try:
        import threading
        connect_result = [None]
        connect_exception = [None]
        
        def try_connect():
            try:
                client = ollama.Client(host=OLLAMA_HOST)
                # Quick test with timeout
                test_result = client.list()
                connect_result[0] = client
            except Exception as e:
                connect_exception[0] = e
        
        connect_thread = threading.Thread(target=try_connect)
        connect_thread.daemon = True
        connect_thread.start()
        connect_thread.join(timeout=1.0)  # 1 second max timeout
        
        if connect_thread.is_alive():
            # Ollama is not responding, mark as unavailable
            ollama_client = None
            OLLAMA_AVAILABLE = False
            OLLAMA_LAST_FAILURE_TIME = time.time()
            return None
        
        if connect_exception[0]:
            # Connection failed, mark as unavailable
            ollama_client = None
            OLLAMA_AVAILABLE = False
            OLLAMA_LAST_FAILURE_TIME = time.time()
            return None
        
        if connect_result[0]:
            ollama_client = connect_result[0]
            OLLAMA_AVAILABLE = True
            return ollama_client
    except Exception as e:
        # Any error means Ollama is unavailable
        ollama_client = None
        OLLAMA_AVAILABLE = False
        OLLAMA_LAST_FAILURE_TIME = time.time()
        return None
    
    return None

# Initial connection attempt - non-blocking, fast fail
# Don't block startup if Ollama is unavailable
try:
    get_ollama_client()
except Exception as e:
    # Silently fail - Ollama is optional
    pass

groq_client = Groq(api_key=GROQ_API_KEY) if GROQ_API_KEY else None
if not groq_client:
    print("âš ï¸ Warning: GROQ_API_KEY not set. Groq models will not be available.")

# OpenRouter client setup
openrouter_client = None
if OPENROUTER_API_KEY:
    openrouter_client = True  # We'll use requests directly for OpenRouter
    print("âœ… OpenRouter API key found.")
else:
    print("âš ï¸ Warning: OPENROUTER_API_KEY not set. OpenRouter models will not be available.")

# Initialize HuggingFace client with explicit provider
try:
    huggingface_client = InferenceClient(
        token=HUGGINGFACE_API_KEY,
        # Don't specify provider, let it auto-detect or use router
    ) if HUGGINGFACE_API_KEY else None
    if not huggingface_client:
        print("âš ï¸ Warning: HUGGINGFACE_API_KEY not set. HuggingFace models will not be available.")
    else:
        print("âœ… HuggingFace Inference API client initialized.")
except Exception as e:
    print(f"âš ï¸ Warning: Failed to initialize HuggingFace client: {e}")
    huggingface_client = None

# DeepSeek client setup (OpenAI-compatible API)
deepseek_client = None
if DEEPSEEK_API_KEY:
    try:
        deepseek_client = OpenAIClient(
            api_key=DEEPSEEK_API_KEY,
            base_url="https://api.deepseek.com"
        )
        print("âœ… DeepSeek API client initialized.")
    except Exception as e:
        print(f"âš ï¸ Warning: Failed to initialize DeepSeek client: {e}")
        deepseek_client = None
else:
    print("âš ï¸ Warning: DEEPSEEK_API_KEY not set. DeepSeek models will not be available.")

# Alibaba DashScope client setup (OpenAI-compatible API)
alibaba_client = None
if ALIBABA_API_KEY:
    try:
        alibaba_client = OpenAIClient(
            api_key=ALIBABA_API_KEY,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
        print("âœ… Alibaba DashScope API client initialized.")
    except Exception as e:
        print(f"âš ï¸ Warning: Failed to initialize Alibaba client: {e}")
        alibaba_client = None
else:
    print("âš ï¸ Warning: ALIBABA_API_KEY not set. Alibaba models will not be available.")

# --- Cross-Encoder Model for Reranking ---
rerank_model = None
RERANK_MODEL_NAME = "cross-encoder/ms-marco-MiniLM-L-6-v2"

def get_rerank_model():
    """Load the cross-encoder model for reranking."""
    global rerank_model
    if rerank_model is None:
        try:
            print(f"ðŸ”„ Loading rerank model: {RERANK_MODEL_NAME}...")
            rerank_model = CrossEncoder(RERANK_MODEL_NAME)
            print(f"âœ… Rerank model '{RERANK_MODEL_NAME}' loaded successfully.")
        except Exception as e:
            print(f"âŒ CRITICAL: Failed to load rerank model '{RERANK_MODEL_NAME}'. Reranking will not be available.")
            print(f"   Error: {e}")
            rerank_model = None
    return rerank_model

# Load model at startup
get_rerank_model()

# --- Output Cleaning Functions ---
def clean_llm_output(raw_output: str, original_query: str = "") -> str:
    """
    DEBUG modda Ã§Ä±ktÄ± temizleyici.
    Her adÄ±mÄ± logluyor.
    """
    print(f"ðŸ”§ [CLEANER] Raw input length: {len(raw_output)}")
    print(f"ðŸ”§ [CLEANER] Raw input: '{raw_output[:100]}...'")
    
    if not raw_output or not raw_output.strip():
        print("ðŸ”§ [CLEANER] Empty input, returning default")
        return "Cevap oluÅŸturulamadÄ±."
    
    try:
        original = raw_output.strip()
        
        # YAKLAÅžIM 1: "Cevap:" sonrasÄ±nÄ± bul - EN Ã–NCELÄ°KLÄ°
        cevap_match = re.search(r'(?i)cevap\s*:?\s*(.+?)$', original, re.MULTILINE | re.DOTALL)
        if cevap_match:
            answer = cevap_match.group(1).strip()
            print(f"ðŸ”§ [CLEANER] Found 'Cevap:' match: '{answer}'")
            if len(answer) > 2:
                clean_answer = answer + ('.' if not answer.endswith('.') else '')
                print(f"ðŸ”§ [CLEANER] Returning cleaned answer: '{clean_answer}'")
                return clean_answer
        
        # YAKLAÅžIM 2: Son satÄ±rlarÄ± kontrol et
        lines = [line.strip() for line in original.split('\n') if line.strip()]
        print(f"ðŸ”§ [CLEANER] Total lines: {len(lines)}")
        
        # Son 3 satÄ±rÄ± kontrol et
        for i, line in enumerate(reversed(lines[-3:])):
            print(f"ðŸ”§ [CLEANER] Checking line {3-i}: '{line}'")
            if (len(line) > 10 and
                not line.startswith(('1.', '2.', '3.', '4.', 'AdÄ±m', 'Ã–NCE', 'SONRA')) and
                not any(word in line.lower() for word in ['analiz', 'kontrol', 'cevaplayacaÄŸÄ±m', 'baÄŸlam'])):
                clean_line = line + ('.' if not line.endswith('.') else '')
                print(f"ðŸ”§ [CLEANER] Returning clean line: '{clean_line}'")
                return clean_line
        
        # YAKLAÅžIM 3: Son noktalÄ± cÃ¼mleyi bul
        sentences = re.findall(r'[^.!?]*[.!?]', original)
        if sentences:
            last_sentence = sentences[-1].strip()
            print(f"ðŸ”§ [CLEANER] Last sentence: '{last_sentence}'")
            if len(last_sentence) > 5:
                return last_sentence
        
        # Son Ã§are
        print("ðŸ”§ [CLEANER] No clean match found, returning last 50 chars")
        return original[-50:] if len(original) > 50 else original
        
    except Exception as e:
        print(f"ðŸ”§ [CLEANER] ERROR: {e}")
        return raw_output.strip()


# --- Helper Functions ---
def is_groq_model(model_name: str) -> bool:
    """Check if the model is intended for Groq based on tested working models."""
    groq_models = [
        "llama-3.1-8b-instant",
        "llama-3.3-70b-versatile",
        "openai/gpt-oss-20b",
        "qwen/qwen3-32b",
        # All 4 models above confirmed working via direct Groq API testing 2025-11-02
        # Keep other potentially working models for future testing
        "allam-2-7b",
        "groq/compound",
        "groq/compound-mini",
        "meta-llama/llama-4-maverick-17b-128e-instruct",
        "meta-llama/llama-4-scout-17b-16e-instruct",
        "meta-llama/llama-guard-4-12b",
        "meta-llama/llama-prompt-guard-2-22m",
        "meta-llama/llama-prompt-guard-2-86m",
        "moonshotai/kimi-k2-instruct",
        "moonshotai/kimi-k2-instruct-0905",
        "openai/gpt-oss-120b",
        "openai/gpt-oss-safeguard-20b"
    ]
    return model_name in groq_models

def is_openrouter_model(model_name: str) -> bool:
    """Check if the model is intended for OpenRouter based on free models."""
    # Free OpenRouter models - cost-effective options
    openrouter_models = [
        "meta-llama/llama-3.1-8b-instruct:free",
        "mistralai/mistral-7b-instruct:free",
        "microsoft/phi-3-mini-4k-instruct:free",
        "google/gemma-2-9b-it:free",
        "google/gemini-2.5-flash-lite",
        "nousresearch/hermes-3-llama-3.1-8b:free",
        "qwen/qwen3-32b"
    ]
    return model_name in openrouter_models

def is_huggingface_model(model_name: str) -> bool:
    """Check if the model is a HuggingFace model."""
    # HuggingFace models typically have format: "organization/model-name"
    # Exclude models that are NOT available on Inference API
    excluded_models = [
        "openai/gpt-oss-20b", "openai/gpt-oss-120b", "openai/gpt-oss-safeguard-20b",
        "teknium/OpenHermes-2.5-Mistral-7B",  # Not available on Inference API
        "meta-llama/Meta-Llama-3-8B-Instruct",  # Not available on Inference API
        "meta-llama/Llama-3-8B-Instruct",  # Not available on Inference API
        "meta-llama/Llama-2-7b-chat-hf",  # May not be available
    ]
    
    # If model is explicitly excluded, it's not a HuggingFace Inference API model
    if model_name in excluded_models:
        return False
    
    # Check if it has the format of a HuggingFace model (contains "/")
    # and is not in the excluded list
    return "/" in model_name and model_name not in excluded_models

def is_deepseek_model(model_name: str) -> bool:
    """Check if the model is a DeepSeek model."""
    deepseek_models = [
        "deepseek-chat",
        "deepseek-reasoner"
    ]
    return model_name in deepseek_models

def is_alibaba_model(model_name: str) -> bool:
    """Check if the model is an Alibaba DashScope model."""
    alibaba_models = [
        "qwen-plus",
        "qwen-turbo",
        "qwen-max",
        "qwen-max-longcontext",
        "qwen-plus-net",
        "qwen-turbo-net"
    ]
    return model_name in alibaba_models

# --- API Endpoints ---
@app.get("/health", summary="Health Check")
def health_check():
    """Check the operational status of the service and its connections."""
    return {
        "status": "ok",
        "ollama_available": OLLAMA_AVAILABLE,
        "groq_available": bool(groq_client),
        "huggingface_available": bool(huggingface_client),
        "openrouter_available": bool(openrouter_client and OPENROUTER_API_KEY),
        "deepseek_available": bool(deepseek_client and DEEPSEEK_API_KEY),
        "alibaba_available": bool(alibaba_client and ALIBABA_API_KEY),
        "ollama_host": OLLAMA_HOST
    }

@app.post("/models/generate", response_model=GenerationResponse, summary="Generate Response from a Model")
async def generate_response(request: GenerationRequest):
    """
    Receives a prompt and a model name, and returns a generated response.
    It dynamically selects the provider (Ollama or Groq) based on the model name.
    """
    model_name = request.model
    prompt = request.prompt

    try:
        if is_alibaba_model(model_name):
            if not alibaba_client or not ALIBABA_API_KEY:
                raise HTTPException(status_code=503, detail="Alibaba client is not available. Check ALIBABA_API_KEY.")

            try:
                chat_completion = alibaba_client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=request.temperature,
                    max_tokens=request.max_tokens,
                    stream=False
                )
                response_content = chat_completion.choices[0].message.content or ""
                return GenerationResponse(response=response_content, model_used=model_name)
            except Exception as e:
                error_str = str(e)
                # Check for authentication errors specifically
                if "401" in error_str or "authentication" in error_str.lower() or "invalid" in error_str.lower():
                    error_detail = f"Alibaba API authentication failed. Please check your ALIBABA_API_KEY environment variable. Error: {error_str}"
                    print(f"âŒ {error_detail}")
                    raise HTTPException(status_code=401, detail=error_detail)
                else:
                    error_detail = f"Alibaba API error: {error_str}"
                    print(f"âŒ {error_detail}")
                    raise HTTPException(status_code=500, detail=error_detail)

        elif is_deepseek_model(model_name):
            if not deepseek_client or not DEEPSEEK_API_KEY:
                raise HTTPException(status_code=503, detail="DeepSeek client is not available. Check DEEPSEEK_API_KEY.")

            try:
                chat_completion = deepseek_client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=request.temperature,
                    max_tokens=request.max_tokens,
                    stream=False
                )
                response_content = chat_completion.choices[0].message.content or ""
                return GenerationResponse(response=response_content, model_used=model_name)
            except Exception as e:
                error_str = str(e)
                # Check for authentication errors specifically
                if "401" in error_str or "authentication" in error_str.lower() or "invalid" in error_str.lower():
                    error_detail = f"DeepSeek API authentication failed. Please check your DEEPSEEK_API_KEY environment variable. Error: {error_str}"
                    print(f"âŒ {error_detail}")
                    raise HTTPException(status_code=401, detail=error_detail)
                else:
                    error_detail = f"DeepSeek API error: {error_str}"
                    print(f"âŒ {error_detail}")
                    raise HTTPException(status_code=500, detail=error_detail)

        elif is_openrouter_model(model_name):
            if not openrouter_client or not OPENROUTER_API_KEY:
                raise HTTPException(status_code=503, detail="OpenRouter client is not available. Check OPENROUTER_API_KEY.")

            # OpenRouter API call
            headers = {
                "Authorization": f"Bearer {OPENROUTER_API_KEY}",
                "Content-Type": "application/json",
                "HTTP-Referer": "http://localhost:3000",  # Optional: for analytics
                "X-Title": "RAG3 Local System"  # Optional: for analytics
            }
            
            payload = {
                "model": model_name,
                "messages": [
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt}
                ],
                "temperature": request.temperature,
                "max_tokens": request.max_tokens
            }
            
            try:
                response = requests.post(
                    "https://openrouter.ai/api/v1/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=60
                )
                
                if response.status_code == 200:
                    result = response.json()
                    if "choices" in result and len(result["choices"]) > 0:
                        response_content = result["choices"][0]["message"]["content"] or ""
                        return GenerationResponse(response=response_content, model_used=model_name)
                    else:
                        raise HTTPException(status_code=500, detail="Invalid response format from OpenRouter")
                else:
                    error_detail = f"OpenRouter API error: {response.status_code}"
                    if response.text:
                        error_detail += f" - {response.text[:500]}"
                    raise HTTPException(status_code=response.status_code, detail=error_detail)
                    
            except requests.exceptions.RequestException as e:
                raise HTTPException(status_code=503, detail=f"OpenRouter API request failed: {str(e)}")

        elif is_groq_model(model_name):
            if not groq_client:
                raise HTTPException(status_code=503, detail="Groq client is not available. Check GROQ_API_KEY.")

            chat_completion = groq_client.chat.completions.create(
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt},
                ],
                model=model_name,
                temperature=request.temperature,
                max_tokens=request.max_tokens,
            )
            response_content = chat_completion.choices[0].message.content or ""
            return GenerationResponse(response=response_content, model_used=model_name)

        elif is_huggingface_model(model_name):
            if not huggingface_client:
                raise HTTPException(status_code=503, detail="HuggingFace client is not available. Check HUGGINGFACE_API_KEY.")

            # Use direct HTTP request to HuggingFace API with multiple endpoint formats
            try:
                print(f"Using direct HTTP request to HuggingFace API for model: {model_name}")
                
                # Try multiple endpoint formats
                # Note: Spaces API format is complex and model-specific, so we skip it
                api_endpoints = [
                    f"https://router.huggingface.co/hf-inference/models/{model_name}",
                    # Removed deprecated endpoint
                ]
                
                headers = {"Content-Type": "application/json"}
                if HUGGINGFACE_API_KEY:
                    headers["Authorization"] = f"Bearer {HUGGINGFACE_API_KEY}"
                
                # Payload for text generation
                payload = {
                    "inputs": prompt,
                    "parameters": {
                        "max_new_tokens": request.max_tokens,
                        "temperature": request.temperature,
                        "return_full_text": False
                    }
                }
                
                last_error = None
                for api_url in api_endpoints:
                    try:
                        print(f"Trying HuggingFace API endpoint: {api_url}")
                        response = requests.post(api_url, json=payload, headers=headers, timeout=120, verify=False)
                        print(f"Response status: {response.status_code}")
                        
                        if response.status_code == 200:
                            result = response.json()
                            # Handle different response formats
                            if isinstance(result, list) and len(result) > 0:
                                if isinstance(result[0], dict) and 'generated_text' in result[0]:
                                    response_content = result[0]['generated_text'].strip()
                                else:
                                    response_content = str(result[0]).strip()
                            elif isinstance(result, dict):
                                if 'generated_text' in result:
                                    response_content = result['generated_text'].strip()
                                elif 'data' in result and len(result['data']) > 0:
                                    # Spaces API format
                                    response_content = str(result['data'][0]).strip()
                                else:
                                    response_content = str(result).strip()
                            else:
                                response_content = str(result).strip()
                            
                            print(f"âœ… Successfully generated response from {api_url}, length: {len(response_content)}")
                            return GenerationResponse(response=response_content, model_used=model_name)
                            
                        elif response.status_code == 503:
                            # Model is loading
                            error_data = response.json() if response.text else {}
                            wait_time = error_data.get('estimated_time', 20)
                            print(f"Model {model_name} is loading on {api_url}, waiting {wait_time} seconds...")
                            time.sleep(min(wait_time, 60))
                            
                            # Retry
                            response = requests.post(api_url, json=payload, headers=headers, timeout=120, verify=False)
                            if response.status_code == 200:
                                result = response.json()
                                if isinstance(result, list) and len(result) > 0:
                                    response_content = result[0].get('generated_text', str(result[0])).strip()
                                else:
                                    response_content = str(result).strip()
                                print(f"âœ… Successfully generated response after retry, length: {len(response_content)}")
                                return GenerationResponse(response=response_content, model_used=model_name)
                            else:
                                error_msg = response.text[:500] if response.text else f"HTTP {response.status_code}"
                                last_error = f"HTTP {response.status_code} after retry: {error_msg}"
                                print(f"âš ï¸ Retry failed: {last_error}")
                        elif response.status_code == 410:
                            # Deprecated endpoint, skip
                            print(f"âš ï¸ Endpoint {api_url} is deprecated (410), trying next...")
                            continue
                        elif response.status_code == 404:
                            # Model not found on this endpoint, try next
                            print(f"âš ï¸ Model not found on {api_url} (404), trying next endpoint...")
                            last_error = f"HTTP 404: Model not found on {api_url}"
                            continue
                        else:
                            error_msg = response.text[:500] if response.text else f"HTTP {response.status_code}"
                            last_error = f"HTTP {response.status_code}: {error_msg}"
                            print(f"âš ï¸ Error on {api_url}: {last_error}")
                            continue
                            
                    except requests.RequestException as req_error:
                        last_error = f"Request failed: {str(req_error)}"
                        print(f"âš ï¸ Request error on {api_url}: {req_error}")
                        continue
                
                # All endpoints failed
                error_detail = (
                    f"HuggingFace API error for model '{model_name}': All endpoints failed. "
                    f"Last error: {last_error or 'Unknown error'}. "
                    f"This model may not be available on HuggingFace Inference API. "
                    f"Some models exist on HuggingFace but are only available via local transformers library, "
                    f"not via Inference API. Some models (like Meta Llama) may require special access or approval. "
                    f"Please check: https://huggingface.co/{model_name} "
                    f"to see if it supports Inference API. "
                    f"Recommended alternatives that work with Inference API: "
                    f"mistralai/Mistral-7B-Instruct-v0.3, Qwen/Qwen2-7B-Instruct, or google/gemma-7b-it"
                )
                print(f"âŒ {error_detail}")
                raise HTTPException(status_code=500, detail=error_detail)
            except HTTPException:
                raise
            except Exception as e:
                error_detail = f"HuggingFace API error for model '{model_name}': {str(e)}"
                print(f"âŒ {error_detail}")
                raise HTTPException(status_code=500, detail=error_detail)

        else: # Default to Ollama
            client = get_ollama_client()
            if client is None:
                raise HTTPException(status_code=503, detail="Ollama client is not available. Check connection to Ollama.")

            # Map common model names to the ones we know are available
            model_mapping = {
                "llama3": "llama3:8b",
                "llama3-8b": "llama3:8b",
                "llama3-70b": "llama3:8b",  # Using 8b as fallback
                "mistral": "mistral:7b",
                "mistral-7b": "mistral:7b",
                "llama3.2": "llama3:8b",  # Using 8b as fallback
                "llama3-8b-8192": "llama3:8b",  # Using 8b as fallback
                "llama3:latest": "llama3:8b",  # Using 8b as fallback
                "mistral:latest": "mistral:7b"  # Using 7b as fallback
            }
            
            # Use the mapped model name if it exists, otherwise use the original
            actual_model_name = model_mapping.get(model_name, model_name)
            
            print(f"Attempting to use model: {actual_model_name}")  # Debug print
            
            response = client.chat(
                model=actual_model_name,
                messages=[{'role': 'user', 'content': prompt}],
                stream=False
            )
            response_content = response['message']['content'] if response.get('message') and response['message'].get('content') else ""
            return GenerationResponse(response=response_content, model_used=actual_model_name)

    except Exception as e:
        # Log the exception details here in a real application
        print(f"âŒ Error during model generation: {e}")
        raise HTTPException(status_code=500, detail=f"An error occurred while generating the response: {str(e)}")

@app.get("/models/available", summary="List Available Models")
def get_available_models():
    """Returns a list of available models from all configured providers."""
    models = {"groq": [], "ollama": [], "huggingface": [], "openrouter": [], "deepseek": [], "alibaba": []}

    if groq_client:
        # Only tested and confirmed working models
        models["groq"] = [
            "llama-3.1-8b-instant",
            "llama-3.3-70b-versatile",
            "openai/gpt-oss-20b",
            "qwen/qwen3-32b",
            # All 4 models above confirmed working via direct Groq API testing 2025-11-02
            # Keep other models for future testing when they become stable
            "groq/compound",
            "groq/compound-mini",
            "allam-2-7b",
            "meta-llama/llama-4-maverick-17b-128e-instruct",
            "meta-llama/llama-4-scout-17b-16e-instruct",
            "openai/gpt-oss-120b",
            "openai/gpt-oss-safeguard-20b",
            "moonshotai/kimi-k2-instruct",
            "moonshotai/kimi-k2-instruct-0905"
        ]

    if openrouter_client and OPENROUTER_API_KEY:
        # Free OpenRouter models - cost-effective and reliable
        models["openrouter"] = [
            "meta-llama/llama-3.1-8b-instruct:free",
            "mistralai/mistral-7b-instruct:free",
            "microsoft/phi-3-mini-4k-instruct:free",
            "google/gemma-2-9b-it:free",
            "google/gemini-2.5-flash-lite",
            "nousresearch/hermes-3-llama-3.1-8b:free",
            "qwen/qwen3-32b"
        ]

    if deepseek_client and DEEPSEEK_API_KEY:
        # DeepSeek models - OpenAI-compatible API
        models["deepseek"] = [
            "deepseek-chat",  # Non-thinking mode (DeepSeek-V3.2-Exp)
            "deepseek-reasoner"  # Thinking mode (DeepSeek-V3.2-Exp)
        ]

    if alibaba_client and ALIBABA_API_KEY:
        # Alibaba DashScope models - OpenAI-compatible API
        models["alibaba"] = [
            "qwen-plus",
            "qwen-turbo",
            "qwen-max",
            "qwen-max-longcontext",
            "qwen-plus-net",
            "qwen-turbo-net"
        ]

    # Try to get Ollama models with fast timeout (non-blocking)
    client = get_ollama_client()
    if client is not None:
        try:
            # Use threading to add timeout to Ollama list() call
            import threading
            installed_models = None
            exception_occurred = None
            
            def fetch_models():
                nonlocal installed_models, exception_occurred
                try:
                    installed_models = client.list()
                except Exception as e:
                    exception_occurred = e
            
            thread = threading.Thread(target=fetch_models)
            thread.daemon = True
            thread.start()
            thread.join(timeout=1.0)  # 1 second timeout - fast fail
            
            if thread.is_alive():
                # Ollama is slow/unavailable, skip it silently
                models["ollama"] = []
            elif exception_occurred:
                # Ollama connection failed, skip it silently
                models["ollama"] = []
            elif installed_models:
                print(f"Ollama list() response: {installed_models}")  # Debug print
                # Fix the model name extraction
                if 'models' in installed_models and isinstance(installed_models['models'], list):
                    model_names = []
                    for model in installed_models['models']:
                        if isinstance(model, dict):
                            # Check for both 'name' and 'model' fields for compatibility
                            if 'name' in model:
                                model_names.append(model['name'])
                            elif 'model' in model:
                                model_names.append(model['model'])
                        elif isinstance(model, str):
                            model_names.append(model)
                        else:
                            # Handle Model objects with 'model' attribute
                            if hasattr(model, 'model'):
                                model_names.append(model.model)
                            elif hasattr(model, 'name'):
                                model_names.append(model.name)
                    models["ollama"] = model_names
                else:
                    # Handle case where the response format is different
                    models["ollama"] = []
                    print(f"Unexpected response format from Ollama list(): {installed_models}")
            else:
                models["ollama"] = []
        except Exception as e:
            print(f"Could not fetch Ollama models: {e}")
            # Return empty list for ollama if it fails during the call
            models["ollama"] = []

    # HuggingFace models - Only models confirmed to work with Inference API
    # Note: Meta Llama models are NOT available on Inference API (require local transformers)
    # Note: Some models exist on HuggingFace but are only available via local transformers
    models["huggingface"] = [
        # Mistral models - Most reliable and confirmed working
        "mistralai/Mistral-7B-Instruct-v0.2",
        "mistralai/Mistral-7B-Instruct-v0.3",
        # Microsoft models
        "microsoft/Phi-3-mini-4k-instruct",
        "microsoft/Phi-3-medium-4k-instruct",
        # Qwen models
        "Qwen/Qwen2-7B-Instruct",
        "Qwen/Qwen2.5-7B-Instruct",
        # Google models
        "google/gemma-7b-it",
        "google/gemma-2-7b-it",
        # Other models
        "tiiuae/falcon-7b-instruct",
        "HuggingFaceH4/zephyr-7b-beta",
        "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
        "Intel/neural-chat-7b-v3-1",
        # Meta Llama models - REMOVED: Not available on Inference API
        # Use mistralai/Mistral-7B-Instruct-v0.3 or Ollama llama3:8b as alternatives
    ]

    return models

@app.get("/models/embedding", summary="List Available Embedding Models")
def get_available_embedding_models():
    """Returns a list of available embedding models from Ollama, HuggingFace, and Alibaba."""
    embedding_models = {
        "ollama": [],
        "huggingface": [],
        "alibaba": []
    }
    
    # Get Ollama embedding models with fast timeout (non-blocking)
    client = get_ollama_client()
    if client is not None:
        try:
            # Use threading for fast timeout
            import threading
            installed_models = None
            exception_occurred = None
            
            def fetch_models():
                nonlocal installed_models, exception_occurred
                try:
                    installed_models = client.list()
                except Exception as e:
                    exception_occurred = e
            
            thread = threading.Thread(target=fetch_models)
            thread.daemon = True
            thread.start()
            thread.join(timeout=1.0)  # 1 second timeout - fast fail
            
            if thread.is_alive() or exception_occurred:
                # Ollama is slow/unavailable, skip it silently
                embedding_models["ollama"] = []
                return embedding_models
            
            if not installed_models:
                embedding_models["ollama"] = []
                return embedding_models
            
            print(f"[Embedding] Ollama list response type: {type(installed_models)}")
            print(f"[Embedding] Ollama list response: {installed_models}")
            
            # Handle different response formats
            models_list = []
            if isinstance(installed_models, dict) and 'models' in installed_models:
                models_list = installed_models['models']
            elif isinstance(installed_models, list):
                models_list = installed_models
            elif hasattr(installed_models, 'models'):
                models_list = installed_models.models
            else:
                # Try to extract models from the response
                models_list = installed_models if isinstance(installed_models, list) else []
            
            # Filter for embedding models (common embedding model names)
            embedding_keywords = ['embed', 'bge', 'nomic', 'instructor', 'e5']
            for model in models_list:
                model_name = ""
                # Handle Model objects (from ollama library)
                if hasattr(model, 'model'):
                    model_name = model.model
                elif hasattr(model, 'name'):
                    model_name = model.name
                # Handle dict format
                elif isinstance(model, dict):
                    model_name = model.get('name', model.get('model', ''))
                # Handle string format
                elif isinstance(model, str):
                    model_name = model
                
                # Check if it's an embedding model
                if model_name and any(keyword in model_name.lower() for keyword in embedding_keywords):
                    embedding_models["ollama"].append(model_name)
                    print(f"[Embedding] Added Ollama embedding model: {model_name}")
            
            print(f"[Embedding] Found {len(embedding_models['ollama'])} Ollama embedding models")
        except Exception as e:
            print(f"[Embedding] Could not fetch Ollama embedding models: {e}")
            import traceback
            traceback.print_exc()
    
    # Popular free HuggingFace embedding models
    embedding_models["huggingface"] = [
        {
            "id": "sentence-transformers/all-MiniLM-L6-v2",
            "name": "all-MiniLM-L6-v2",
            "description": "En popÃ¼ler, hÄ±zlÄ± ve kÃ¼Ã§Ã¼k (384 boyut)",
            "dimensions": 384,
            "language": "en"
        },
        {
            "id": "sentence-transformers/all-mpnet-base-v2",
            "name": "all-mpnet-base-v2",
            "description": "YÃ¼ksek kalite (768 boyut)",
            "dimensions": 768,
            "language": "en"
        },
        {
            "id": "BAAI/bge-small-en-v1.5",
            "name": "bge-small-en-v1.5",
            "description": "BAAI BGE - Ä°yi kalite, kÃ¼Ã§Ã¼k boyut",
            "dimensions": 384,
            "language": "en"
        },
        {
            "id": "BAAI/bge-base-en-v1.5",
            "name": "bge-base-en-v1.5",
            "description": "BAAI BGE - Daha iyi kalite (768 boyut)",
            "dimensions": 768,
            "language": "en"
        },
        {
            "id": "intfloat/multilingual-e5-small",
            "name": "multilingual-e5-small",
            "description": "Ã‡ok dilli destek (384 boyut)",
            "dimensions": 384,
            "language": "multilingual"
        },
        {
            "id": "intfloat/multilingual-e5-base",
            "name": "multilingual-e5-base",
            "description": "Ã‡ok dilli destek - YÃ¼ksek kalite (768 boyut)",
            "dimensions": 768,
            "language": "multilingual"
        },
        {
            "id": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            "name": "paraphrase-multilingual-MiniLM-L12-v2",
            "description": "Ã‡ok dilli parafraz (384 boyut)",
            "dimensions": 384,
            "language": "multilingual"
        },
        {
            "id": "sentence-transformers/all-MiniLM-L12-v2",
            "name": "all-MiniLM-L12-v2",
            "description": "Daha bÃ¼yÃ¼k MiniLM (384 boyut)",
            "dimensions": 384,
            "language": "en"
        },
        {
            "id": "sentence-transformers/paraphrase-MiniLM-L6-v2",
            "name": "paraphrase-MiniLM-L6-v2",
            "description": "Parafraz iÃ§in optimize (384 boyut)",
            "dimensions": 384,
            "language": "en"
        },
        {
            "id": "sentence-transformers/distiluse-base-multilingual-cased-v1",
            "name": "distiluse-base-multilingual-cased-v1",
            "description": "Ã‡ok dilli DistilUSE (512 boyut)",
            "dimensions": 512,
            "language": "multilingual"
        }
    ]
    
    # Alibaba DashScope embedding models
    if alibaba_client and ALIBABA_API_KEY:
        embedding_models["alibaba"] = [
            {
                "id": "text-embedding-v4",
                "name": "text-embedding-v4",
                "description": "Alibaba DashScope - En gÃ¼ncel embedding modeli (1024 boyut, Ã§ok dilli)",
                "dimensions": 1024,
                "language": "multilingual"
            },
            {
                "id": "text-embedding-v3",
                "name": "text-embedding-v3",
                "description": "Alibaba DashScope - Ã–nceki versiyon (1024 boyut, Ã§ok dilli)",
                "dimensions": 1024,
                "language": "multilingual"
            },
            {
                "id": "text-embedding-v2",
                "name": "text-embedding-v2",
                "description": "Alibaba DashScope - Stabil versiyon (1536 boyut, Ã§ok dilli)",
                "dimensions": 1536,
                "language": "multilingual"
            }
        ]
    
    return embedding_models

@app.get("/debug/models", summary="Debug: List All Models with Details")
def debug_get_all_models():
    """Returns detailed information about all available models."""
    result = {
        "ollama_available": OLLAMA_AVAILABLE,
        "groq_available": bool(groq_client),
        "models": {}
    }

    if OLLAMA_AVAILABLE and ollama_client is not None:
        try:
            raw_response = ollama_client.list()
            result["models"]["ollama_raw"] = raw_response
            result["models"]["ollama_processed"] = []
            
            if 'models' in raw_response and isinstance(raw_response['models'], list):
                for model in raw_response['models']:
                    result["models"]["ollama_processed"].append(model)
        except Exception as e:
            result["ollama_error"] = str(e)

    if groq_client:
        result["models"]["groq"] = [
            "llama-3.1-8b-instant",
            "llama-3.3-70b-versatile",
            "openai/gpt-oss-20b",
            "qwen/qwen3-32b",
            # All 4 models above confirmed working via direct Groq API testing 2025-11-02
            "groq/compound",
            "groq/compound-mini",
            "allam-2-7b",
            "meta-llama/llama-4-maverick-17b-128e-instruct",
            "meta-llama/llama-4-scout-17b-16e-instruct",
            "openai/gpt-oss-120b",
            "openai/gpt-oss-safeguard-20b",
            "moonshotai/kimi-k2-instruct",
            "moonshotai/kimi-k2-instruct-0905"
        ]

    return result

@app.post("/models/pull", response_model=PullModelResponse, summary="Pull a Model into Ollama")
async def pull_model(request: PullModelRequest):
    """
    Pulls a model into Ollama if it's not already available.
    """
    model_name = request.model_name
    
    client = get_ollama_client()
    if client is None:
        raise HTTPException(status_code=503, detail="Ollama client is not available. Check connection to Ollama.")
    
    try:
        # Check if model is already available
        installed_models = client.list()
        model_names = []
        for model in installed_models['models']:
            if isinstance(model, dict):
                if 'name' in model:
                    model_names.append(model['name'])
                elif 'model' in model:
                    model_names.append(model['model'])
            elif hasattr(model, 'model'):
                model_names.append(model.model)
            elif hasattr(model, 'name'):
                model_names.append(model.name)
        
        if model_name in model_names:
            return PullModelResponse(
                status="success", 
                message=f"Model {model_name} is already available"
            )
        
        # Pull the model
        print(f"Pulling model {model_name}...")
        # Use the client's pull method
        response = client.pull(model_name)
        print(f"Model {model_name} pulled successfully")
        
        return PullModelResponse(
            status="success", 
            message=f"Model {model_name} pulled successfully"
        )
        
    except Exception as e:
        print(f"âŒ Error during model pull: {e}")
        raise HTTPException(status_code=500, detail=f"An error occurred while pulling the model: {str(e)}")

def is_huggingface_embedding_model(model_name: str) -> bool:
    """Check if the model is a HuggingFace embedding model."""
    # HuggingFace models typically have format: "organization/model-name" or contain "/"
    hf_embedding_models = [
        "sentence-transformers/all-MiniLM-L6-v2",
        "sentence-transformers/all-mpnet-base-v2",
        "BAAI/bge-small-en-v1.5",
        "BAAI/bge-base-en-v1.5",
        "intfloat/multilingual-e5-small",
        "intfloat/multilingual-e5-base",
        "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        "sentence-transformers/all-MiniLM-L12-v2",
        "sentence-transformers/paraphrase-MiniLM-L6-v2",
        "sentence-transformers/distiluse-base-multilingual-cased-v1",
    ]
    # Check if it's in the list or contains "/" (typical HuggingFace format)
    return model_name in hf_embedding_models or ("/" in model_name and not model_name.startswith("openai/"))

@app.post("/embed", response_model=EmbedResponse, summary="Generate Embeddings for Texts")
async def generate_embeddings(request: EmbedRequest):
    """
    Receives a list of texts and returns their embeddings.
    Uses HuggingFace if a HuggingFace model is specified, otherwise tries Ollama first.
    """
    texts = request.texts
    
    if not texts:
        raise HTTPException(status_code=400, detail="No texts provided for embedding.")
    
    try:
        start_time = time.time()
        default_model = os.getenv("DEFAULT_EMBEDDING_MODEL", "nomic-embed-text")
        model_name = getattr(request, "model", None) or default_model
        
        # Clean model name (remove :latest, :v1, etc.)
        if ":" in model_name:
            model_name = model_name.split(":")[0]
        
        print(f"Generating embeddings for {len(texts)} texts using model: {model_name}")
        
        embeddings = []
        
        # Skip Ollama if a HuggingFace model is explicitly requested
        # Check if it's a HuggingFace model by format (contains / and starts with known orgs)
        hf_orgs = ["sentence-transformers/", "intfloat/", "BAAI/"]
        is_hf_model = "/" in model_name and any(model_name.startswith(org) for org in hf_orgs)
        skip_ollama = is_huggingface_embedding_model(model_name) or is_hf_model
        
        if skip_ollama:
            print(f"Detected HuggingFace model '{model_name}', skipping Ollama")
        else:
            # Try Ollama first if available and not a HuggingFace model
            client = get_ollama_client()
            if client is not None:
                try:
                    for text in texts:
                        response = client.embeddings(model=model_name, prompt=text)
                        
                        if isinstance(response, dict):
                            embedding = response.get('embedding')
                        else:
                            embedding = getattr(response, 'embedding', None)

                        if embedding is not None:
                            embeddings.append(embedding)
                        else:
                            raise Exception(f"Could not extract embedding from Ollama response.")
                    
                    end_time = time.time()
                    processing_time = end_time - start_time
                    print(f"âœ… Successfully generated {len(embeddings)} embeddings using Ollama in {processing_time:.2f} seconds.")
                    return EmbedResponse(embeddings=embeddings, model_used=model_name)
                except Exception as ollama_error:
                    print(f"âš ï¸ Ollama embedding failed: {ollama_error}. Trying HuggingFace fallback...")
        
        # Use HuggingFace if Ollama is not available, failed, or if a HuggingFace model was explicitly requested
        # Map Ollama embedding models to HuggingFace equivalents
        ollama_to_hf_mapping = {
            "nomic-embed-text": "sentence-transformers/all-MiniLM-L6-v2",
            "nomic-embed": "sentence-transformers/all-MiniLM-L6-v2",
            "embeddinggemma": "sentence-transformers/all-MiniLM-L6-v2",  # Use reliable model
            "bge-m3": "BAAI/bge-small-en-v1.5",  # Similar to BGE
            "default": "sentence-transformers/all-MiniLM-L6-v2"
        }
        
        # Popular free HuggingFace embedding models (ordered by popularity and quality)
        hf_embedding_models = [
            "sentence-transformers/all-MiniLM-L6-v2",  # Most popular, 384 dim, fast
            "sentence-transformers/all-mpnet-base-v2",  # Better quality, 768 dim
            "BAAI/bge-small-en-v1.5",  # Popular, good quality
            "intfloat/multilingual-e5-small",  # Multilingual support
            "intfloat/multilingual-e5-base",  # Multilingual support, better quality
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"  # Multilingual
        ]
        
        # Determine which HuggingFace model to use
        if model_name in ollama_to_hf_mapping:
            hf_embedding_model = ollama_to_hf_mapping[model_name]
        elif model_name in hf_embedding_models:
            hf_embedding_model = model_name
        else:
            # Default to most popular and reliable model
            hf_embedding_model = hf_embedding_models[0]
        
        print(f"Using HuggingFace embedding model: {hf_embedding_model}")
        
        # Try models in order of preference until one works
        last_error = None
        for attempt_model in [hf_embedding_model] + [m for m in hf_embedding_models if m != hf_embedding_model]:
            try:
                print(f"Attempting to use HuggingFace model: {attempt_model}")
                
                # Use direct API calls to HuggingFace Inference API with BATCH processing
                # This is more reliable than using InferenceClient for embeddings
                # Process all texts in a single batch request instead of individual requests
                api_endpoints = [
                    f"https://router.huggingface.co/hf-inference/models/{attempt_model}/pipeline/feature-extraction",
                    # Old router format as fallback
                    f"https://router.huggingface.co/hf-inference/pipeline/feature-extraction/{attempt_model}",
                    # Removed deprecated legacy API endpoint
                ]
                
                # Try with API key first, then without if it fails with 403 or 401
                use_api_key = bool(HUGGINGFACE_API_KEY)
                last_request_error = None
                embedding_response = None
                response = None
                
                for api_url in api_endpoints:
                    try:
                        headers = {"Content-Type": "application/json"}
                        if use_api_key and HUGGINGFACE_API_KEY:
                            headers["Authorization"] = f"Bearer {HUGGINGFACE_API_KEY}"
                        
                        # HuggingFace feature-extraction API supports batch processing
                        # Send all texts in one request for better performance
                        payload = {"inputs": texts}
                        
                        print(f"Calling HuggingFace API: {api_url} (batch size: {len(texts)}, with API key: {use_api_key})")
                        # Use increased timeout for batch processing - 5 minutes for large batches
                        response = requests.post(api_url, json=payload, headers=headers, timeout=300, verify=False)
                        print(f"Response status: {response.status_code} for {api_url}")
                        
                        # Handle 410 (deprecated endpoint) - skip old endpoints immediately
                        if response.status_code == 410:
                            error_msg = response.text[:500] if response.text else "Endpoint deprecated"
                            print(f"âš ï¸ Endpoint deprecated (410): {error_msg}")
                            last_request_error = f"HTTP {response.status_code}: {error_msg}"
                            continue  # Try next endpoint
                        
                        # Track if we retried without API key
                        retried_without_key = False
                        
                        # Handle 401 (unauthorized) - try without API key
                        if response.status_code == 401 and use_api_key:
                            print(f"âš ï¸ Unauthorized (401) with API key, trying without API key...")
                            headers_no_key = {"Content-Type": "application/json"}
                            response = requests.post(api_url, json=payload, headers=headers_no_key, timeout=300, verify=False)
                            print(f"Response without API key (401->): {response.status_code}")
                            retried_without_key = True
                        
                        # Handle 403 (permission denied) - try without API key
                        if response.status_code == 403 and use_api_key:
                            print(f"âš ï¸ API key permission denied (403), trying without API key...")
                            headers_no_key = {"Content-Type": "application/json"}
                            response = requests.post(api_url, json=payload, headers=headers_no_key, timeout=300, verify=False)
                            print(f"Response without API key (403->): {response.status_code}")
                            retried_without_key = True
                        
                        # Handle 503 (model loading) with retry - after auth handling
                        retry_count = 0
                        current_headers = headers_no_key if retried_without_key else headers
                        while response.status_code == 503 and retry_count < 3:
                            print(f"Model {attempt_model} is loading, waiting 10 seconds... (attempt {retry_count + 1}/3)")
                            time.sleep(10)
                            response = requests.post(api_url, json=payload, headers=current_headers, timeout=300, verify=False)
                            print(f"Retry response status: {response.status_code}")
                            retry_count += 1
                        
                        if response.status_code == 200:
                            embedding_response = response.json()
                            print(f"âœ… Successfully got batch embeddings from {api_url}")
                            
                            # Process batch response - should be list of embeddings
                            if isinstance(embedding_response, list):
                                # Batch response: list of embeddings for each input
                                for emb in embedding_response:
                                    if isinstance(emb, list):
                                        embeddings.append([float(x) for x in emb])
                                    else:
                                        print(f"âš ï¸ Unexpected embedding format: {type(emb)}")
                                        embeddings.append([float(emb)] if isinstance(emb, (int, float)) else [0.0])
                            else:
                                print(f"âš ï¸ Unexpected batch response format: {type(embedding_response)}")
                                # Fallback: treat as single embedding
                                if hasattr(embedding_response, 'tolist'):
                                    embeddings.append(embedding_response.tolist())
                                else:
                                    embeddings.append(list(embedding_response) if hasattr(embedding_response, '__iter__') else [0.0])
                            break  # Success, exit endpoint loop
                        
                        elif response.status_code == 404:
                            error_msg = response.text[:500] if response.text else "No error message"
                            print(f"âŒ Failed with status {response.status_code}: {error_msg}")
                            last_request_error = f"HTTP {response.status_code}: {error_msg}"
                            continue
                        else:
                            error_msg = response.text[:500] if response.text else "No error message"
                            print(f"âŒ Failed with status {response.status_code}: {error_msg}")
                            last_request_error = f"HTTP {response.status_code}: {error_msg}"
                            continue  # Try next endpoint
                            
                    except requests.RequestException as req_error:
                        last_request_error = f"Request failed: {str(req_error)}"
                        continue  # Try next endpoint
                
                # Check if we got successful batch response
                if len(embeddings) == 0 or len(embeddings) != len(texts):
                    error_msg = f"Batch processing failed: expected {len(texts)} embeddings, got {len(embeddings)}"
                    if last_request_error:
                        error_msg += f". Last error: {last_request_error}"
                    raise Exception(error_msg)
                
                # Success - update the model name used
                hf_embedding_model = attempt_model
                break
                
            except Exception as hf_error:
                last_error = hf_error
                print(f"âš ï¸ HuggingFace model {attempt_model} failed: {hf_error}. Trying next model...")
                embeddings = []  # Reset for next attempt
                continue
        
        if len(embeddings) == 0:
            error_detail = f"Failed to generate embeddings with all HuggingFace models."
            if last_error:
                error_detail += f" Last error: {str(last_error)}"
            if skip_ollama:
                error_detail += f" (Ollama was skipped because HuggingFace model '{model_name}' was requested. ALL embeddings must use the same model to ensure consistent similarity scores. Please retry or choose a different model.)"
            raise HTTPException(status_code=500, detail=error_detail)
        
        end_time = time.time()
        processing_time = end_time - start_time
        print(f"âœ… Successfully generated {len(embeddings)} embeddings using HuggingFace in {processing_time:.2f} seconds.")
        
        return EmbedResponse(embeddings=embeddings, model_used=hf_embedding_model)
    
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Error during embedding generation: {e}")
        raise HTTPException(status_code=500, detail=f"An error occurred while generating embeddings: {str(e)}")

# Replace the query endpoint implementation with a proper one
@app.post("/query", response_model=RAGQueryResponse, summary="Handle RAG Query")
async def handle_rag_query(request: RAGQueryRequest):
    """
    Handle RAG queries by generating a response based on the query.
    Since this service doesn't have access to vector stores, it will generate
    a response indicating that RAG functionality should be handled by a dedicated service.
    """
    # Generate a response using the model inference capabilities
    try:
        # Use the existing generation endpoint to create a response
        generation_request = GenerationRequest(
            prompt=f"Answer the following question: {request.query}",
            model=request.model or "llama3:8b",
            temperature=0.7,
            max_tokens=512
        )
        
        # Call the generation function directly instead of using await
        response = await generate_response(generation_request)
        
        return RAGQueryResponse(
            answer=response.response,
            sources=[]  # No sources since we don't have RAG capabilities here
        )
    except Exception as e:
        # Fallback response
        return RAGQueryResponse(
            answer="I'm a lightweight model inference service and don't have access to the full RAG pipeline. "
                   "Please ensure you're connecting to the correct RAG service.",
            sources=[]
        )

@app.post("/rerank", response_model=RerankResponse, summary="Rerank Documents")
async def rerank_documents(request: RerankRequest):
    """
    Reranks a list of documents based on their relevance to a query
    using a cross-encoder model.
    """
    model = get_rerank_model()
    if model is None:
        raise HTTPException(status_code=503, detail="Rerank model is not available.")

    try:
        # The model expects a list of (query, document) pairs
        model_input = [(request.query, doc) for doc in request.documents]
        
        # Compute scores
        scores = model.predict(model_input)
        
        # Combine documents with their scores
        results = []
        for i, score in enumerate(scores):
            results.append({
                "document": request.documents[i],
                "index": i,
                "relevance_score": float(score)
            })
            
        # Sort results by score in descending order
        results.sort(key=lambda x: x["relevance_score"], reverse=True)
        
        return RerankResponse(results=results)

    except Exception as e:
        print(f"âŒ Error during reranking: {e}")
        raise HTTPException(status_code=500, detail=f"An error occurred during reranking: {str(e)}")


@app.post("/generate-answer", response_model=GenerationResponse, summary="Generate Answer from Documents")
async def generate_answer_from_docs(request: GenerateAnswerRequest):
    """
    Generates an answer to a query based on a provided list of documents.
    """
    try:
        context_texts = [doc.get('content', '') or doc.get('text', '') for doc in request.docs]
        context_str = "\n\n".join(context_texts)

        if len(context_str) > request.max_context_chars:
            context_str = context_str[:request.max_context_chars] + "..."

        system_prompt = (
            "Sen Ã¶ÄŸrencilere yardÄ±mcÄ± olan bir eÄŸitim asistanÄ±sÄ±n.\n\n"
            "KURALLAR:\n"
            "â€¢ Verilen ders materyalindeki bilgileri kullanarak soruyu TÃœRKÃ‡E cevapla\n"
            "â€¢ Ders materyalinde bilgi varsa: Madde madde, net ve anlaÅŸÄ±lÄ±r ÅŸekilde aÃ§Ä±kla\n"
            "â€¢ Ders materyalinde bilgi yoksa: 'Bu konu, verilen ders materyallerinde bulunmuyor' de\n"
            "â€¢ KÄ±sa ve Ã¶ÄŸretici ol, gereksiz aÃ§Ä±klama yapma\n"
        )
        
        full_prompt = f"""DERS MATERYALÄ°:
{context_str}

SORU: {request.query}

CEVAP:"""

        generation_request = GenerationRequest(
            prompt=full_prompt,
            model=request.model,
            temperature=0.3,  # Reduced from 0.7 to 0.3 for more accurate, context-faithful answers
            max_tokens=1024
        )
        
        # Ham cevabÄ± al
        raw_response = await generate_response(generation_request)
        
        # CLEANER DEVRE DIÅžI - Ham cevabÄ± olduÄŸu gibi dÃ¶ndÃ¼r
        # Ã‡Ã¼nkÃ¼ cleaner yanlÄ±ÅŸ satÄ±rlarÄ± seÃ§iyor ve cevabÄ± bozuyor
        # cleaned_answer = clean_llm_output(raw_response.response, request.query)
        
        return GenerationResponse(response=raw_response.response, model_used=raw_response.model_used)

    except Exception as e:
        print(f"âŒ Error in generate_answer_from_docs: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Failed to generate answer: {str(e)}")


if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8002))
    uvicorn.run("main:app", host="0.0.0.0", port=port, reload=True)